# -*- coding: utf-8 -*-
"""object detection and labelling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11oRPOOv0KjdfGq7Y_3lQzsG8CSX8APGY
"""

import cv2
import torch

# Commented out IPython magic to ensure Python compatibility.
# ! git clone https://github.com/ultralytics/yolov5.git
# %cd yolov5
# ! pip install -r requirements.txt

model = torch.hub.load("ultralytics/yolov5", "yolov5s", pretrained = True)

"""# **VIDEO 01 (cakes)**"""

input_path = "/content/drive/MyDrive/Colab Notebooks/Deep Learning/Object Detection and Labelling/input and output files/01 input (cakes).mp4"
output_path = "/content/drive/MyDrive/Colab Notebooks/Deep Learning/Object Detection and Labelling/input and output files/01 output.avi"

cap = cv2.VideoCapture(input_path)
fps = cap.get(cv2.CAP_PROP_FPS)
frame_width, frame_height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
forcc = cv2.VideoWriter_fourcc(*"mp4v")
out = cv2.VideoWriter(output_path, forcc, fps, (frame_width, frame_height))

print(cap)
print(fps)
print(frame_width, frame_height)

# Processing each frame
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Perform object detection
    results = model(frame)  # YOLOv5 interface

    # Extract detections
    detections = results.pandas().xyxy[0]   # Bounding box, classes, and confidence

    for _, detection in detections.iterrows():
        x1, y1, x2, y2 = int(detection["xmin"]), int(detection["ymin"]), int(detection["xmax"]), int(detection["ymax"])
        confidence = detection["confidence"]
        label = f"{detection['name']} {confidence:.2f}"  # Create the label string

        # Draw bounding box and label
        color = (0, 255, 0)  # Green color
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)


    # Write frame to the output video
    out.write(frame)

    # Show frame (optional)
    # cv2.imshow("YOLLOv5 Object Detection and Labelling", frame)
    # if cv2.waitkey(1) & 0xFF == ord("q"):
    #     break

# Release resourses
cap.release()
out.release()
cv2.destroyAllWindows()

"""# **VIDEO 02 (icons)**"""

input_path = "/content/drive/MyDrive/Colab Notebooks/Deep Learning/Object Detection and Labelling/input and output files/02 input (icons).mp4"
output_path = "/content/drive/MyDrive/Colab Notebooks/Deep Learning/Object Detection and Labelling/input and output files/02 output.avi"

cap = cv2.VideoCapture(input_path)
fps = cap.get(cv2.CAP_PROP_FPS)
frame_width, frame_height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
forcc = cv2.VideoWriter_fourcc(*"mp4v")
out = cv2.VideoWriter(output_path, forcc, fps, (frame_width, frame_height))

print(cap)
print(fps)
print(frame_width, frame_height)

# Processing each frame
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Perform object detection
    results = model(frame)  # YOLOv5 interface

    # Extract detections
    detections = results.pandas().xyxy[0]   # Bounding box, classes, and confidence

    for _, detection in detections.iterrows():
        x1, y1, x2, y2 = int(detection["xmin"]), int(detection["ymin"]), int(detection["xmax"]), int(detection["ymax"])
        confidence = detection["confidence"]
        label = f"{detection['name']} {confidence:.2f}"  # Create the label string

        # Draw bounding box and label
        color = (0, 255, 0)  # Green color
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)


    # Write frame to the output video
    out.write(frame)

    # Show frame (optional)
    # cv2.imshow("YOLLOv5 Object Detection and Labelling", frame)
    # if cv2.waitkey(1) & 0xFF == ord("q"):
    #     break

# Release resourses
cap.release()
out.release()
cv2.destroyAllWindows()

"""# **VIDEO 03 (vehicles)**"""

input_path = "/content/drive/MyDrive/Colab Notebooks/Deep Learning/Object Detection and Labelling/input and output files/03 input (vehicles).mp4"
output_path = "/content/drive/MyDrive/Colab Notebooks/Deep Learning/Object Detection and Labelling/input and output files/03 output.avi"

cap = cv2.VideoCapture(input_path)
fps = cap.get(cv2.CAP_PROP_FPS)
frame_width, frame_height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
forcc = cv2.VideoWriter_fourcc(*"mp4v")
out = cv2.VideoWriter(output_path, forcc, fps, (frame_width, frame_height))

print(cap)
print(fps)
print(frame_width, frame_height)

# Processing each frame
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Perform object detection
    results = model(frame)  # YOLOv5 interface

    # Extract detections
    detections = results.pandas().xyxy[0]   # Bounding box, classes, and confidence

    for _, detection in detections.iterrows():
        x1, y1, x2, y2 = int(detection["xmin"]), int(detection["ymin"]), int(detection["xmax"]), int(detection["ymax"])
        confidence = detection["confidence"]
        label = f"{detection['name']} {confidence:.2f}"  # Create the label string

        # Draw bounding box and label
        color = (0, 255, 0)  # Green color
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)


    # Write frame to the output video
    out.write(frame)

    # Show frame (optional)
    # cv2.imshow("YOLLOv5 Object Detection and Labelling", frame)
    # if cv2.waitkey(1) & 0xFF == ord("q"):
    #     break

# Release resourses
cap.release()
out.release()
cv2.destroyAllWindows()

"""# **VIDEO 04 (traffic)**"""

input_path = "/content/drive/MyDrive/Colab Notebooks/Deep Learning/Object Detection and Labelling/input and output files/04 input (traffic).mp4"
output_path = "/content/drive/MyDrive/Colab Notebooks/Deep Learning/Object Detection and Labelling/input and output files/04 output.avi"

cap = cv2.VideoCapture(input_path)
fps = cap.get(cv2.CAP_PROP_FPS)
frame_width, frame_height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
forcc = cv2.VideoWriter_fourcc(*"mp4v")
out = cv2.VideoWriter(output_path, forcc, fps, (frame_width, frame_height))

print(cap)
print(fps)
print(frame_width, frame_height)

# Processing each frame
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Perform object detection
    results = model(frame)  # YOLOv5 interface

    # Extract detections
    detections = results.pandas().xyxy[0]   # Bounding box, classes, and confidence

    for _, detection in detections.iterrows():
        x1, y1, x2, y2 = int(detection["xmin"]), int(detection["ymin"]), int(detection["xmax"]), int(detection["ymax"])
        confidence = detection["confidence"]
        label = f"{detection['name']} {confidence:.2f}"  # Create the label string

        # Draw bounding box and label
        color = (0, 255, 0)  # Green color
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)


    # Write frame to the output video
    out.write(frame)

    # Show frame (optional)
    # cv2.imshow("YOLLOv5 Object Detection and Labelling", frame)
    # if cv2.waitkey(1) & 0xFF == ord("q"):
    #     break

# Release resourses
cap.release()
out.release()
cv2.destroyAllWindows()

"""# **VIDEO 05 (front desk)**"""

input_path = "/content/drive/MyDrive/Colab Notebooks/Deep Learning/Object Detection and Labelling/input and output files/05 input (front desk).mp4"
output_path = "/content/drive/MyDrive/Colab Notebooks/Deep Learning/Object Detection and Labelling/input and output files/05 output.avi"

cap = cv2.VideoCapture(input_path)
fps = cap.get(cv2.CAP_PROP_FPS)
frame_width, frame_height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
forcc = cv2.VideoWriter_fourcc(*"mp4v")
out = cv2.VideoWriter(output_path, forcc, fps, (frame_width, frame_height))

print(cap)
print(fps)
print(frame_width, frame_height)

# Processing each frame
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Perform object detection
    results = model(frame)  # YOLOv5 interface

    # Extract detections
    detections = results.pandas().xyxy[0]   # Bounding box, classes, and confidence

    for _, detection in detections.iterrows():
        x1, y1, x2, y2 = int(detection["xmin"]), int(detection["ymin"]), int(detection["xmax"]), int(detection["ymax"])
        confidence = detection["confidence"]
        label = f"{detection['name']} {confidence:.2f}"  # Create the label string

        # Draw bounding box and label
        color = (0, 255, 0)  # Green color
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)


    # Write frame to the output video
    out.write(frame)

    # Show frame (optional)
    # cv2.imshow("YOLLOv5 Object Detection and Labelling", frame)
    # if cv2.waitkey(1) & 0xFF == ord("q"):
    #     break

# Release resourses
cap.release()
out.release()
cv2.destroyAllWindows()