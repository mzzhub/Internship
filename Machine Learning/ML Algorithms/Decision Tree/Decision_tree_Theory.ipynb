{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d4ab94f9",
      "metadata": {
        "id": "d4ab94f9"
      },
      "source": [
        "# **Decision Tree**\n",
        "* Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems\n",
        "* But mostly it is preferred for solving Classification problems.\n",
        "* It is a tree-structured classifier, where\n",
        "    * **Internal nodes** represent the features of a dataset\n",
        "    * **Branches** represent the decision rules\n",
        "    * Each **leaf node** represents the outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c1d888c",
      "metadata": {
        "id": "0c1d888c"
      },
      "source": [
        "**In a Decision tree, there are two nodes,**\n",
        "1. **Decision Node :** used to make any decision and have multiple branches\n",
        "2. **Leaf Node :** They are the output of those decisions and do not contain any further branches."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "783b5831",
      "metadata": {
        "id": "783b5831"
      },
      "source": [
        "**Why use Decision Trees?**\n",
        "* Decision Trees usually mimic human thinking ability while making a decision, so it is easy to understand.\n",
        "* The logic behind the decision tree can be easily understood because it shows a tree-like structure."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5543d219",
      "metadata": {
        "id": "5543d219"
      },
      "source": [
        "**Decision Tree Terminologies**\n",
        "1. **Root Node:**\n",
        "    * Root node is from where the decision tree starts.\n",
        "    * It represents the entire dataset, which further gets divided into two or more homogeneous sets.\n",
        "2. **Leaf Node:**\n",
        "    * Leaf nodes are the final output node.\n",
        "    * The tree cannot be segregated further after getting a leaf node.\n",
        "3. **Splitting:**\n",
        "    * Splitting is the process of dividing the decision node/root node into sub-nodes according to the given conditions.\n",
        "4. **Branch/Sub Tree:**\n",
        "    * A tree formed by splitting the tree.\n",
        "5. **Pruning:**\n",
        "    * Pruning is the process of removing the unwanted branches from the tree.\n",
        "6. **Parent/Child node:**\n",
        "    * The root node of the tree is called the **parent node.**\n",
        "    * And other nodes are called the **child nodes.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bfd8284",
      "metadata": {
        "id": "9bfd8284"
      },
      "source": [
        "**How does the Decision Tree algorithm Work?**\n",
        "* In a decision tree, for predicting the class of the given dataset, the algorithm starts from the root node of the tree.\n",
        "* This algorithm compares the values of root attribute with the record (real dataset) attribute and, based on the comparison, follows the branch and jumps to the next node.\n",
        "* For the next node, the algorithm again compares the attribute value with the other sub-nodes and move further.\n",
        "* It continues the process until it reaches the leaf node of the tree."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0205178",
      "metadata": {
        "id": "e0205178"
      },
      "source": [
        "**Step-1:** Begin the tree with the root node, says $S$, which contains the complete dataset.\n",
        "\n",
        "**Step-2:** Find the best attribute in the dataset using *Attribute Selection Measure* (ASM).\n",
        "\n",
        "**Step-3:** Divide the $S$ into subsets that contains possible values for the best attributes.\n",
        "\n",
        "**Step-4:** Generate the decision tree node, which contains the best attribute.\n",
        "\n",
        "**Step-5:** Recursively make new decision trees using the subsets of the dataset created in **Step-3**. Continue this process until a stage is reached where you cannot further classify the nodes and called the *final node* as a leaf node."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a10b111",
      "metadata": {
        "id": "2a10b111"
      },
      "source": [
        "**Attribute Selection Measures**\n",
        "\n",
        "* While implementing a Decision tree, the main issue arises that how to select the best attribute for the root node and for sub-nodes.\n",
        "* So, to solve such problems there is a technique which is called as *Attribute selection measure* or **ASM**. By this measurement, we can easily select the best attribute for the nodes of the tree.\n",
        "* There are two popular techniques for ASM,\n",
        "    1. Information Gain\n",
        "    2. Gini Index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ba25d40",
      "metadata": {
        "id": "5ba25d40"
      },
      "source": [
        "**Information Gain:**\n",
        "\n",
        "* Information gain is the measurement of changes in *entropy* after the segmentation of a dataset based on an attribute.\n",
        "\n",
        "* It calculates how much information a feature provides us about a class.\n",
        "\n",
        "* According to the value of information gain, we split the node and build the decision tree.\n",
        "\n",
        "* A decision tree algorithm always tries to maximize the value of information gain, and a node/attribute having the highest\n",
        "information gain is split first. It can be calculated using the below formula:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The formula for **Information Gain** is:\n",
        "\n",
        "$$\n",
        "\\text{Information Gain}(A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\times \\text{Entropy}(S_v)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- **Entropy(S)** is the entropy of the original set \\( $S$ \\).\n",
        "- \\( $A$ \\) is the attribute being evaluated.\n",
        "- \\( $S_v$ \\) is the subset of \\( $S$ \\) where attribute \\( $A$ \\) has value \\( $v$ \\).\n",
        "- \\( $\\frac{|S_v|}{|S|}$ \\) is the proportion of examples in \\( $S$ \\) where attribute \\( $A$ \\) takes the value \\( $v$ \\).\n",
        "- **Entropy( $S_v$ )** is the entropy of the subset \\( $S_v$ \\).\n"
      ],
      "metadata": {
        "id": "L7WkTWPhnxIe"
      },
      "id": "L7WkTWPhnxIe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Entropy:**\n",
        "* Entropy is a metric to measure the impurity in a given attribute.\n",
        "* It specifies randomness in data.\n",
        "* Entropy can be calculated as:"
      ],
      "metadata": {
        "id": "hhajLjavpFlx"
      },
      "id": "hhajLjavpFlx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\text{Entropy}(S) = -P(\\text{yes}) \\times \\log_2 (P(\\text{yes})) - P(\\text{no}) \\times \\log_2 (P(\\text{no}))\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $S$ is the total number of samples.\n",
        "- $P(\\text{yes})$ is the probability of a positive outcome \"yes\".\n",
        "- $P(\\text{no})$ is the probability of a negative outcome \"no\".\n"
      ],
      "metadata": {
        "id": "Pb2BpIs6pBRp"
      },
      "id": "Pb2BpIs6pBRp"
    },
    {
      "cell_type": "markdown",
      "id": "74c102e4",
      "metadata": {
        "id": "74c102e4"
      },
      "source": [
        "# **2. Gini Index:**\n",
        "Gini index is a measure of impurity or purity used while creating a decision tree in the **CART** (Classification and Regression Tree) algorithm.\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3fd9cae",
      "metadata": {
        "id": "f3fd9cae"
      },
      "source": [
        "* **Pruning:** Getting an Optimal Decision tree\n",
        "\n",
        "* Pruning is a process of deleting the unnecessary nodes from a tree in order to get the optimal decision tree.\n",
        "\n",
        "* A too-large tree increases the risk of overfitting, and a small tree may not capture all the important features of the dataset.\n",
        "* Therefore, a technique that decreases the size of the learning tree without reducing accuracy is known as Pruning."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}